{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim=input_vocab_size, output_dim=embedding_dim, mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.lstm = layers.LSTM(units=hidden_dim, return_state=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.embedding(inputs)\n",
    "        encoder_out, state_h, state_c = self.lstm(x)\n",
    "        return (state_h, state_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seq_len = 20\n",
    "embedding_dim = 10\n",
    "target_vocab_size = 50\n",
    "input_vocab_size = 30\n",
    "hidden_dim = 16\n",
    "\n",
    "X = tf.random.uniform(shape=(batch_size, seq_len), minval=0, maxval=input_vocab_size)\n",
    "y = tf.random.uniform(shape=(batch_size, seq_len), minval=0, maxval=target_vocab_size)\n",
    "encoder = Encoder(\n",
    "    input_vocab_size=input_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "encoder(X)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            target_vocab_size, embedding_dim, mask_zero=True\n",
    "        )\n",
    "\n",
    "        self.lstm = layers.LSTM(hidden_dim, return_state=True, return_sequences=True)\n",
    "\n",
    "        self.dense = layers.Dense(target_vocab_size, \"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        decoder_input, encoder_output = inputs\n",
    "\n",
    "        x = self.embedding(decoder_input)\n",
    "\n",
    "        decoder_output, _, _ = self.lstm(x, initial_state=encoder_output)\n",
    "\n",
    "        return self.dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 20, 50), dtype=float32, numpy=\n",
       "array([[[0.02004724, 0.02001585, 0.01997572, ..., 0.02002917,\n",
       "         0.02002766, 0.01985104],\n",
       "        [0.01997081, 0.0200408 , 0.01991662, ..., 0.020055  ,\n",
       "         0.02002386, 0.01993472],\n",
       "        [0.0198773 , 0.02004897, 0.0198836 , ..., 0.02004115,\n",
       "         0.02006204, 0.02008183],\n",
       "        ...,\n",
       "        [0.02005896, 0.02004887, 0.01997051, ..., 0.02008729,\n",
       "         0.01996567, 0.01996697],\n",
       "        [0.02009996, 0.02006258, 0.01994334, ..., 0.01999923,\n",
       "         0.01998866, 0.0200236 ],\n",
       "        [0.0200354 , 0.02011444, 0.01984122, ..., 0.01993513,\n",
       "         0.02001637, 0.02015435]],\n",
       "\n",
       "       [[0.01986672, 0.02007607, 0.01995894, ..., 0.01995959,\n",
       "         0.01996633, 0.0200538 ],\n",
       "        [0.01996388, 0.0200776 , 0.01994407, ..., 0.01988657,\n",
       "         0.0199767 , 0.0200689 ],\n",
       "        [0.01994152, 0.02012059, 0.01984973, ..., 0.0198383 ,\n",
       "         0.01999843, 0.02017463],\n",
       "        ...,\n",
       "        [0.0199283 , 0.02008826, 0.01985196, ..., 0.0199445 ,\n",
       "         0.02000697, 0.02010192],\n",
       "        [0.01983727, 0.02001813, 0.01990299, ..., 0.02004357,\n",
       "         0.02002744, 0.020071  ],\n",
       "        [0.01988808, 0.0200489 , 0.01988309, ..., 0.02001144,\n",
       "         0.02003474, 0.0199729 ]],\n",
       "\n",
       "       [[0.02001546, 0.01999434, 0.0199463 , ..., 0.02007365,\n",
       "         0.01994802, 0.02005192],\n",
       "        [0.01995979, 0.02004472, 0.01991317, ..., 0.02008031,\n",
       "         0.01998014, 0.02008558],\n",
       "        [0.02001074, 0.02002233, 0.01997057, ..., 0.02009047,\n",
       "         0.01991564, 0.02000603],\n",
       "        ...,\n",
       "        [0.0199892 , 0.02005077, 0.01992557, ..., 0.01995827,\n",
       "         0.02006436, 0.0200151 ],\n",
       "        [0.01999253, 0.02004826, 0.01994281, ..., 0.01996579,\n",
       "         0.02012756, 0.02001861],\n",
       "        [0.0200093 , 0.02004165, 0.01994641, ..., 0.01993309,\n",
       "         0.02010208, 0.02004651]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.02005387, 0.01993615, 0.02004279, ..., 0.02003155,\n",
       "         0.0200245 , 0.01997405],\n",
       "        [0.02003587, 0.01998118, 0.02003697, ..., 0.0200961 ,\n",
       "         0.01999278, 0.01999394],\n",
       "        [0.01998529, 0.02003934, 0.01992381, ..., 0.02001153,\n",
       "         0.02000993, 0.02012091],\n",
       "        ...,\n",
       "        [0.02010657, 0.02011811, 0.01985533, ..., 0.01987303,\n",
       "         0.02004972, 0.02004396],\n",
       "        [0.01999392, 0.02011781, 0.01982925, ..., 0.01987782,\n",
       "         0.02008292, 0.0201657 ],\n",
       "        [0.01987986, 0.02016575, 0.01979558, ..., 0.01994073,\n",
       "         0.02004427, 0.02020106]],\n",
       "\n",
       "       [[0.0198331 , 0.01999525, 0.02001262, ..., 0.02005575,\n",
       "         0.02004265, 0.02006534],\n",
       "        [0.01992919, 0.02002931, 0.01996872, ..., 0.02002194,\n",
       "         0.0200333 , 0.02000002],\n",
       "        [0.01995889, 0.02007376, 0.01990286, ..., 0.01990661,\n",
       "         0.02006733, 0.020064  ],\n",
       "        ...,\n",
       "        [0.01988529, 0.01995741, 0.0200161 , ..., 0.02004098,\n",
       "         0.02004755, 0.02002132],\n",
       "        [0.01993368, 0.01992276, 0.02008315, ..., 0.02001942,\n",
       "         0.01995035, 0.01996895],\n",
       "        [0.02001942, 0.0199676 , 0.02004566, ..., 0.01998126,\n",
       "         0.01998774, 0.01996172]],\n",
       "\n",
       "       [[0.02003586, 0.02009051, 0.01987024, ..., 0.01998871,\n",
       "         0.01997889, 0.0199757 ],\n",
       "        [0.01994279, 0.02008677, 0.01985758, ..., 0.01996184,\n",
       "         0.02001176, 0.02009827],\n",
       "        [0.01997707, 0.0200898 , 0.01986126, ..., 0.01992129,\n",
       "         0.01997355, 0.02003804],\n",
       "        ...,\n",
       "        [0.01991   , 0.0200227 , 0.01989774, ..., 0.02002388,\n",
       "         0.01999869, 0.02005394],\n",
       "        [0.01994497, 0.02004711, 0.01988755, ..., 0.01996669,\n",
       "         0.01997477, 0.02001425],\n",
       "        [0.01989686, 0.02002998, 0.01991259, ..., 0.02001262,\n",
       "         0.02002048, 0.02003832]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "decoder((X, encoder(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        encoder_output = self.encoder(encoder_input)\n",
    "        decoder_output = self.decoder((decoder_input, encoder_output))\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder)\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 50), dtype=int64, numpy=\n",
       "array([[18, 10, 16, ..., 15,  4, 19],\n",
       "       [16,  2,  7, ..., 18,  8,  4],\n",
       "       [ 3, 14,  9, ...,  8, 18, 14],\n",
       "       ...,\n",
       "       [16, 19,  0, ...,  7, 18, 19],\n",
       "       [12,  5, 18, ...,  9,  8,  0],\n",
       "       [ 3,  6, 11, ...,  8,  7,  9]], dtype=int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(model((X, X)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.9121\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.9121\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9120\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.9118\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.9116\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9114\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9112\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9109\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.9107\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9104\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.9101\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9098\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9094\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.9091\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.9087\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.9084\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9080\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.9076\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9072\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.9067\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.9063\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.9058\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9053\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9048\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.9043\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.9037\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.9032\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9027\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.9021\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9015\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.9010\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9004\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8998\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8991\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.8984\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8977\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8969\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8962\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.8954\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8945\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.8937\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.8928\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3.8919\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8909\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.8900\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.8890\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8880\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.8870\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8860\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8850\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8839\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8828\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.8818\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8807\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8796\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.8785\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8774\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.8763\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8751\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8739\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8727\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.8715\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8702\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.8689\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8676\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.8662\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.8649\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8635\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8621\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.8607\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.8592\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.8578\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8563\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.8548\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.8532\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.8517\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8501\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.8485\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8468\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8452\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8435\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.8418\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8400\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.8383\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8366\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8349\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8332\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8314\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8296\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8278\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8260\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8242\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8223\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8205\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8186\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.8168\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.8149\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8131\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.8112\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc935cd410>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.random.uniform(\n",
    "    (batch_size, input_vocab_size), minval=0, maxval=input_vocab_size, dtype=tf.int32\n",
    ")\n",
    "target_data = tf.random.uniform(\n",
    "    (batch_size, target_vocab_size), minval=0, maxval=target_vocab_size, dtype=tf.int32\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [input_data, target_data[:, :-1]], target_data[:, 1:], epochs=100, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
